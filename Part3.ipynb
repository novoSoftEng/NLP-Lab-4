{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae480894-a9b0-4559-bfd6-77cb4e3e817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tf_keras.optimizers import Adam,SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "import logging\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3de60-b2f4-4b19-814d-f776a0f7a496",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03dfa1e-4d24-46b5-af3d-3a9c29b77906",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the path to your JSON Lines file\n",
    "file_path = 'All_Beauty_5.json'\n",
    "\n",
    "# Initialize a list to store the JSON objects\n",
    "json_objects = []\n",
    "\n",
    "# Open the file and read it line by line\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Strip any extra whitespace and parse the JSON object\n",
    "        json_object = json.loads(line.strip())\n",
    "        json_objects.append(json_object)\n",
    "\n",
    "# Optionally, convert the list of JSON objects into a pandas DataFrame\n",
    "df = pd.DataFrame(json_objects)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de7dc755-1a22-4d62-b2ca-c5de76fec2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>09 1, 2016</td>\n",
       "      <td>A3CIUOJXQ5VDQ2</td>\n",
       "      <td>B0000530HU</td>\n",
       "      <td>{'Size:': ' 7.0 oz', 'Flavor:': ' Classic Ice ...</td>\n",
       "      <td>Shelly F</td>\n",
       "      <td>As advertised. Reasonably priced</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1472688000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>11 14, 2013</td>\n",
       "      <td>A3H7T87S984REU</td>\n",
       "      <td>B0000530HU</td>\n",
       "      <td>{'Size:': ' 7.0 oz', 'Flavor:': ' Classic Ice ...</td>\n",
       "      <td>houserules18</td>\n",
       "      <td>Like the oder and the feel when I put it on my...</td>\n",
       "      <td>Good for the face</td>\n",
       "      <td>1384387200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>08 18, 2013</td>\n",
       "      <td>A3J034YH7UG4KT</td>\n",
       "      <td>B0000530HU</td>\n",
       "      <td>{'Size:': ' 7.0 oz', 'Flavor:': ' Classic Ice ...</td>\n",
       "      <td>Adam</td>\n",
       "      <td>I bought this to smell nice after I shave.  Wh...</td>\n",
       "      <td>Smells awful</td>\n",
       "      <td>1376784000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>05 3, 2011</td>\n",
       "      <td>A2UEO5XR3598GI</td>\n",
       "      <td>B0000530HU</td>\n",
       "      <td>{'Size:': ' 7.0 oz', 'Flavor:': ' Classic Ice ...</td>\n",
       "      <td>Rich K</td>\n",
       "      <td>HEY!! I am an Aqua Velva Man and absolutely lo...</td>\n",
       "      <td>Truth is There IS Nothing Like an AQUA VELVA MAN.</td>\n",
       "      <td>1304380800</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>05 6, 2011</td>\n",
       "      <td>A3SFRT223XXWF7</td>\n",
       "      <td>B00006L9LC</td>\n",
       "      <td>{'Size:': ' 200ml/6.7oz'}</td>\n",
       "      <td>C. C. Christian</td>\n",
       "      <td>If you ever want to feel pampered by a shampoo...</td>\n",
       "      <td>Bvlgari Shampoo</td>\n",
       "      <td>1304640000</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0      5.0      True   09 1, 2016  A3CIUOJXQ5VDQ2  B0000530HU   \n",
       "1      5.0      True  11 14, 2013  A3H7T87S984REU  B0000530HU   \n",
       "2      1.0      True  08 18, 2013  A3J034YH7UG4KT  B0000530HU   \n",
       "3      5.0     False   05 3, 2011  A2UEO5XR3598GI  B0000530HU   \n",
       "4      5.0      True   05 6, 2011  A3SFRT223XXWF7  B00006L9LC   \n",
       "\n",
       "                                               style     reviewerName  \\\n",
       "0  {'Size:': ' 7.0 oz', 'Flavor:': ' Classic Ice ...         Shelly F   \n",
       "1  {'Size:': ' 7.0 oz', 'Flavor:': ' Classic Ice ...     houserules18   \n",
       "2  {'Size:': ' 7.0 oz', 'Flavor:': ' Classic Ice ...             Adam   \n",
       "3  {'Size:': ' 7.0 oz', 'Flavor:': ' Classic Ice ...           Rich K   \n",
       "4                          {'Size:': ' 200ml/6.7oz'}  C. C. Christian   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0                   As advertised. Reasonably priced   \n",
       "1  Like the oder and the feel when I put it on my...   \n",
       "2  I bought this to smell nice after I shave.  Wh...   \n",
       "3  HEY!! I am an Aqua Velva Man and absolutely lo...   \n",
       "4  If you ever want to feel pampered by a shampoo...   \n",
       "\n",
       "                                             summary  unixReviewTime vote  \\\n",
       "0                                         Five Stars      1472688000  NaN   \n",
       "1                                  Good for the face      1384387200  NaN   \n",
       "2                                       Smells awful      1376784000  NaN   \n",
       "3  Truth is There IS Nothing Like an AQUA VELVA MAN.      1304380800   25   \n",
       "4                                    Bvlgari Shampoo      1304640000    3   \n",
       "\n",
       "  image  \n",
       "0   NaN  \n",
       "1   NaN  \n",
       "2   NaN  \n",
       "3   NaN  \n",
       "4   NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c40ffd-cf33-4fe0-a9b8-faf75f6028dc",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fdf322-beb6-4962-b019-05e5d7b5e5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/unamed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/unamed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/unamed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0d660c-abc3-44e8-bb45-f5caf1657f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize necessary components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876c7599-fd7a-42e3-82c0-c7eedd099835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Stop words removal\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    tokens_stemmed = [ps.stem(word) for word in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens_lemmatized = [lemmatizer.lemmatize(word) for word in tokens_stemmed]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    return ' '.join(tokens_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d27e04-48f7-4182-a5f5-4f09d9880153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep only relevant columns\n",
    "data = df\n",
    "data[\"reviewText\"] = data[\"reviewText\"].apply(preprocess_text)\n",
    "data[\"overall\"] = data[\"overall\"].apply(float)\n",
    "df['overall'] = df['overall'] - 1\n",
    "# Drop rows with missing values\n",
    "data.dropna(subset=['reviewText', 'overall'], inplace=True)\n",
    "data[\"overall\"]=data[\"overall\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c71839b4-c81e-4fc0-92d8-6a09274ef7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train, validation\n",
    "training_sentences, test_sentences, training_labels, test_labels = train_test_split(data[\"reviewText\"], data[\"overall\"], test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97bc07d7-c629-4484-95d7-96ae2ac5b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "if None in training_sentences:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93caeffd-6770-490b-94db-75ade87d76fe",
   "metadata": {},
   "source": [
    "### Tokenizationokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b2562-5e31-451a-a402-2aab5a8ba65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e54152c-9c42-4ccc-8f37-2dd1f4b82d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(training_sentences.to_list(),\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "\n",
    "validation_encodings = tokenizer(test_sentences.to_list(),\n",
    "                            truncation=True,\n",
    "                            padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dcfe99-e26e-44bf-8200-b832f9c1e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the input encodings and labels into a TensorFlow Dataset object\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "                            dict(train_encodings),\n",
    "                            training_labels\n",
    "                            ));\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "                            dict(validation_encodings),\n",
    "                            test_labels\n",
    "                            ));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d5363-c54f-40f0-8fe9-5b9cd507df2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39399ce9-ea66-4e4d-aa09-f41860e4074f",
   "metadata": {},
   "source": [
    "## BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491781c6-616d-415e-a6f2-e87b2edc9fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6db87ac7-57a4-4ec3-aec8-5895b8f1726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b2a2b193-5167-4536-bc6f-5b33cd0e9a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased',num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8aeb9d0b-338b-41f0-abc3-550ce5c95cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =  Adam(learning_rate=2e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.hf_compute_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cdf647-cea7-4300-abbd-faa85b840e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset.shuffle(100).batch(8),\n",
    "          epochs=3,\n",
    "          batch_size=8,\n",
    "          validation_data=validation_dataset.shuffle(100).batch(8), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b753c7d-9cc6-4e75-9045-b81431f58e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# We plot train and validation accuracy\n",
    "\n",
    "ax = plt.figure().gca()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bfb5d3-d9e9-49ac-82d9-6817fd1ad549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the model and then evaluate it on holdout set\n",
    "\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained(\"./output_model\")\n",
    "result = model.evaluate(holdout_dataset.batch(8))\n",
    "dict(zip(model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959ed94-9305-4469-8ea0-c84caedd443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that we create the confusion matrix of our predictions\n",
    "\n",
    "cm = tf.math.confusion_matrix(\n",
    "    holdout_labels, pred_label, num_classes=2, weights=None, dtype=tf.dtypes.int32,\n",
    "    name=None\n",
    ").numpy()\n",
    "\n",
    "print(\"confusion matrix\\n\",cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
